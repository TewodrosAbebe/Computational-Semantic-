# Wolayyta Word Embedding

Semantic similarity measures are the ability to compare various terms, such as words and sentences. By comparing words that are conceptually related but not necessarily lexically related, semantic similarity metrics can be used to determine the degree of significance between the two sets of words. Different natural language processing applications use semantic similarity measures (NLP). The vagueness of words, the complexity of real languages, and other issues make calculating semantic similarity difficult. Despite their similarity in meaning, the words are not lexicographically interchangeable.

In many natural language processing tasks, distributed language representation has become the most widely applied technique for language representation. The majority of deep learning-based natural language processing models rely on pre-trained distributed word representations, also known as word embeddings. For such models, determining the most qualitative word embeddings is critical. We look at the widely used word2vec and fasttext methods for building distributed word representations. We perform an entrinsic word evaluation of word2vec and fasttext using the resources we built for Wolaytta.

The following datasets are useful resources for evaluating the performance of word embedding in terms of determining the most qualitativeseveral state-of-the-art word embedding approaches.

## Wolaytta WordSim-100: 

Thirteen language speakers re-annotated WordSim353 word pairs in accordance with the original guidelines.

[Wolaytta wordsim_100 Dataset](https://github.com/TewodrosAbebe/Computational-Semantic-/blob/master/wol-WordSim-100.txt)

## Wolaytta 1000-wolaytta analogy dataset:

[Wolaytta analogy Dataset](https://github.com/TewodrosAbebe/Computational-Semantic-/blob/master/Analogy-Datasets.txt)


# Code with evaluation

[Wolaytta Word Embedding](https://github.com/TewodrosAbebe/Computational-Semantic-/blob/master/Wolaytta_Word_Embedding.ipynb)

https://www.overleaf.com/project/61b36129a48e46292d910e0b
