{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wolaytta Word Embedding with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings \n",
    "\n",
    "A word embedding is an approach to provide a dense vector representation of words that capture something about their meaning. The vector space representation of the words provides a projection where words with similar meanings are locally clustered within the space. It is modern approach for representing text in natural language processing. Embedding algorithms like Word2Vec and GloVe are key to the state-of-the-art results achieved by neural network models on natural language processing problems like machine translation.\n",
    "\n",
    "Wolaytta is poorly resourced and highly inflected language from Afro-asiatic language family. This is the first attempt to share a Wolaytta small text corpus, Word Embedding, and wolaytta wordsim100. The Wolaytta-word-embedding is a pre-trained distributed word representation, wordsim100 - provides human annotated scores of relatedness between term pairs collected form potential users which was used to evaluate word embedding model.\n",
    "\n",
    "Things to see:\n",
    "\n",
    "1. Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gensim is an open source Python library for natural language processing\n",
    "import gensim, logging\n",
    "import os\n",
    "## from gensim.models import FastText\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xgebt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/xgebt/anaconda3/lib/python3.8/site-packages (0.1.95)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "RAW_DATA_DIR = './data'\n",
    "CLEAN_DATA_DIR = './clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2880 files created to ./clean'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "file_count = 1\n",
    "saved_files = os.listdir(CLEAN_DATA_DIR)\n",
    "if len(saved_files)>0:\n",
    "    file_count = len(saved_files)\n",
    "else:        \n",
    "    for fname in os.listdir(RAW_DATA_DIR):\n",
    "        try:\n",
    "            if fname.endswith(\".txt\"):\n",
    "                txt_file = open(os.path.join(RAW_DATA_DIR, fname),encoding='utf8').readlines()\n",
    "                sen = []\n",
    "                for s in txt_file:\n",
    "                    sent = sent_tokenize(s)\n",
    "                    for st in sent:\n",
    "                        st = st.lower()\n",
    "                        b = re.sub(r'\\w+7\\w+', '\\'', st)\n",
    "                        a = ('\\n'+ b.replace('”', '\\'\\'').replace('’', '\\'').replace('7', '\\'').replace('_', '').replace('-', ''))\n",
    "                        c = re.sub(r'\\d+', 'NUM', a)\n",
    "                        words = re.findall(r'\\w+\\'*\\w+', c)\n",
    "                        ss = ' '.join(x for x in words if x)\n",
    "                        if ss not in sen:\n",
    "                            sen.append(ss.encode(\"ascii\", \"ignore\").decode())\n",
    "                new_fname = f\"{file_count}.txt\"\n",
    "                clean_file = open(os.path.join(CLEAN_DATA_DIR, new_fname),'w+',encoding='utf8')\n",
    "                sen_per_line_test = ''\n",
    "                for s in sen:\n",
    "                    sen_per_line_test = sen_per_line_test +'\\n'+s\n",
    "                clean_file.write(sen_per_line_test)\n",
    "\n",
    "                file_count+=1\n",
    "        except:\n",
    "            continue\n",
    "f\"{file_count} files created to {CLEAN_DATA_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./subword-model/all_in_one.txt'):\n",
    "    for fname in os.listdir(CLEAN_DATA_DIR):\n",
    "        txt_file = open(os.path.join(CLEAN_DATA_DIR, fname),encoding='utf8').readlines()\n",
    "        clean_file = open(os.path.join('./subword-model', 'all_in_one.txt'),'a+',encoding='utf8')\n",
    "        for s in txt_file:\n",
    "            clean_file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based \n",
    "#text generation systems where the vocabulary size is predetermined prior to the neural model training. \n",
    "#SentencePiece implements subword units BPE and unigram language model.\n",
    "#It treats the sentences just as sequences of Unicode characters. \n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--input=' + os.path.join('./subword-model', 'all_in_one.txt') +\n",
    "            ' --model_prefix=wol_sp --vocab_size=16000 --hard_vocab_limit=false')\n",
    "\n",
    "#It is a unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
    "#unigram_model_trainer extracts frequent sub strings...\n",
    "# Then save file at /Wolaytta_Word_Embedding.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁hi', 'ggi', 'yya', 'ppe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"./subword-model/wol_sp.model\")\n",
    "sp.EncodeAsPieces(\"higgiyyappe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines: 863006 words: 6310966 characters: 50965677\n"
     ]
    }
   ],
   "source": [
    "#Open Wolaytta sentence list\n",
    "file = open(\"./subword-model/all_in_one.txt\", \"r\", encoding=\"utf8\")\n",
    "number_of_lines = 0\n",
    "number_of_words = 0\n",
    "number_of_characters = 0\n",
    "for line in file:\n",
    "    line = line.strip(\"\\n\")\n",
    "    words = line.split()\n",
    "    number_of_lines += 1\n",
    "    number_of_words += len(words)\n",
    "    number_of_characters += len(line)\n",
    "file.close()\n",
    "\n",
    "print(\"lines:\", number_of_lines, \"words:\", number_of_words, \"characters:\", number_of_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of tokenized list of sentences\n",
    "#wordList = []\n",
    "#for word in file.readlines():\n",
    "#z = re.findall('\\w+\\'*\\w+', word) \n",
    "    #print(z)\n",
    "   # wordList.append(z)\n",
    "#for a in wordList:\n",
    "   # print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9f2608ee65dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Tokens in text file :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wordList' is not defined"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens in text file :', len(wordList), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "EMBEDDING_DIR='./emb_models'\n",
    "PREPROCESSED_DIR='./clean'\n",
    "class WEConfig(object):\n",
    "    \"\"\"Training parameters\"\"\"\n",
    "    window=5 #Maximum skip length window between words\n",
    "    emb_dim=50 # Set size of word vectors\n",
    "    emb_lr=0.05 #learning rate for SGD estimation.\n",
    "    nepoach=20 #number of training epochs\n",
    "    nthread=20 #number of training threads\n",
    "    sample = 0 #Set threshold for occurrence of words. Those that appear with higher frequency in the training data will be randomly down-sampled\n",
    "    negative = 15 #negative sampling is used with defined negative example\n",
    "    hs = 0 #0 Use Hierarchical Softmax; default is 0 (not used)\n",
    "    binary=0 # 0 means not saved as .bin. Change to 1 if allowed to binary format\n",
    "    sg=1 # 0 means CBOW model is used. Change to 1 to use Skip-gram model\n",
    "    iterate=10 # Run more training iterations\n",
    "    minFreq=2 #This will discard words that appear less than minFreq times \n",
    "    WORD_VECTOR_CACHE=EMBEDDING_DIR+'wol_word_vectors_sts.npy'\n",
    "    if sg==0:\n",
    "      model_name='wol_fasttext_cbow_'+str(emb_dim)+'D'\n",
    "    elif sg==1:\n",
    "      model_name='wol_fasttext_sg_'+str(emb_dim)+'D'\n",
    "    \n",
    "class corpus_sentences(object):# accept sentence stored one per line in list of files inside defined directory\n",
    "    def __init__(self, dirname, sub_word = True):\n",
    "        self.dirname = dirname\n",
    "        self.sub_word = sub_word\n",
    "        \n",
    "        if self.sub_word:\n",
    "            self.sp=spm.SentencePieceProcessor()\n",
    "            self.sp_model=self.sp.Load(\"./subword-model/wol_sp.model\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname),encoding='utf8'):\n",
    "                 if self.sub_word:\n",
    "                    yield self.sp.EncodeAsPieces(line)\n",
    "                 else:\n",
    "                    yield line.split()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wol_model=load_wol_word_vectors(sub_word = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v_model(sub_word = True):\n",
    "    print('Loading Sentences with memory freindly iterator ...\\n')\n",
    "    sentences = corpus_sentences(PREPROCESSED_DIR, sub_word) # a memory-friendly iterator \n",
    "    if WEConfig.sg==0:\n",
    "        model_type='CBOW'\n",
    "    else:\n",
    "        model_type='Skip-gram'\n",
    "    if sub_word:\n",
    "        model_name=f'subword_wol_W2V_{model_type}_{WEConfig.emb_dim}D'\n",
    "    else:\n",
    "        model_name=f'wol_W2V_{model_type}_{WEConfig.emb_dim}D'\n",
    "    print('Training Sentence Piece Word2Vec '+model_type+' with '+str(WEConfig.emb_dim)+' dimension\\n') \n",
    "    _model = Word2Vec(sentences, vector_size=WEConfig.emb_dim, window=WEConfig.window, \n",
    "                            min_count=WEConfig.minFreq, workers=WEConfig.nthread,sg=WEConfig.sg,\n",
    "                            epochs=WEConfig.iterate,negative=WEConfig.negative,\n",
    "                            hs=WEConfig.hs,sorted_vocab=1)  \n",
    "    _model.build_vocab(sentences)\n",
    "    #trim unneeded model memory = use (much) less RAM\n",
    "    _model.init_sims(replace=True)\n",
    "    \n",
    "    #Saving model   \n",
    "    model_path=os.path.join(EMBEDDING_DIR,model_name)\n",
    "    _model.save(model_path)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FastText_model(sub_word = True):\n",
    "    print('Loading Sentences with memory freindly iterator ...\\n')\n",
    "    sentences = corpus_sentences(PREPROCESSED_DIR, sub_word) # a memory-friendly iterator \n",
    "    if WEConfig.sg==0:\n",
    "        model_type='CBOW'\n",
    "    else:\n",
    "        model_type='Skip-gram'\n",
    "    if sub_word:\n",
    "        model_name=f'subword_wol_FastText_{model_type}_{WEConfig.emb_dim}D'\n",
    "    else:\n",
    "        model_name=f'wol_FastText_{model_type}_{WEConfig.emb_dim}D'\n",
    "    print('Training Sentence Piece Word2Vec '+model_type+' with '+str(WEConfig.emb_dim)+' dimension\\n') \n",
    "    _model = FastText(sentences, vector_size=WEConfig.emb_dim, window=WEConfig.window, \n",
    "                            min_count=WEConfig.minFreq, workers=WEConfig.nthread,sg=WEConfig.sg,\n",
    "                            epochs=WEConfig.iterate,negative=WEConfig.negative,\n",
    "                            hs=WEConfig.hs,sorted_vocab=1)\n",
    "    _model.build_vocab(sentences)\n",
    "\n",
    "    #trim unneeded model memory = use (much) less RAM\n",
    "    _model.init_sims(replace=True)\n",
    "    \n",
    "    #Saving model   \n",
    "    model_path=os.path.join(EMBEDDING_DIR,model_name)\n",
    "    _model.save(model_path)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:56:49,352 : INFO : collecting all words and their counts\n",
      "2021-11-29 11:56:49,356 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-29 11:56:49,461 : INFO : PROGRESS: at sentence #10000, processed 94367 words, keeping 25860 word types\n",
      "2021-11-29 11:56:49,485 : INFO : PROGRESS: at sentence #20000, processed 137135 words, keeping 34975 word types\n",
      "2021-11-29 11:56:49,500 : INFO : PROGRESS: at sentence #30000, processed 157135 words, keeping 43701 word types\n",
      "2021-11-29 11:56:49,513 : INFO : PROGRESS: at sentence #40000, processed 177135 words, keeping 52299 word types\n",
      "2021-11-29 11:56:49,527 : INFO : PROGRESS: at sentence #50000, processed 197135 words, keeping 60914 word types\n",
      "2021-11-29 11:56:49,540 : INFO : PROGRESS: at sentence #60000, processed 217135 words, keeping 69543 word types\n",
      "2021-11-29 11:56:49,554 : INFO : PROGRESS: at sentence #70000, processed 237135 words, keeping 77508 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentences with memory freindly iterator ...\n",
      "\n",
      "Training Sentence Piece Word2Vec Skip-gram with 50 dimension\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:56:49,568 : INFO : PROGRESS: at sentence #80000, processed 257135 words, keeping 84909 word types\n",
      "2021-11-29 11:56:49,583 : INFO : PROGRESS: at sentence #90000, processed 277135 words, keeping 90915 word types\n",
      "2021-11-29 11:56:49,598 : INFO : PROGRESS: at sentence #100000, processed 296511 words, keeping 95135 word types\n",
      "2021-11-29 11:56:49,643 : INFO : PROGRESS: at sentence #110000, processed 398973 words, keeping 103853 word types\n",
      "2021-11-29 11:56:49,682 : INFO : PROGRESS: at sentence #120000, processed 495786 words, keeping 109784 word types\n",
      "2021-11-29 11:56:49,703 : INFO : PROGRESS: at sentence #130000, processed 541473 words, keeping 110742 word types\n",
      "2021-11-29 11:56:49,713 : INFO : PROGRESS: at sentence #140000, processed 555554 words, keeping 110894 word types\n",
      "2021-11-29 11:56:49,729 : INFO : PROGRESS: at sentence #150000, processed 582222 words, keeping 111943 word types\n",
      "2021-11-29 11:56:49,765 : INFO : PROGRESS: at sentence #160000, processed 672357 words, keeping 116120 word types\n",
      "2021-11-29 11:56:49,801 : INFO : PROGRESS: at sentence #170000, processed 761326 words, keeping 121419 word types\n",
      "2021-11-29 11:56:49,834 : INFO : PROGRESS: at sentence #180000, processed 845239 words, keeping 123122 word types\n",
      "2021-11-29 11:56:49,870 : INFO : PROGRESS: at sentence #190000, processed 942845 words, keeping 125640 word types\n",
      "2021-11-29 11:56:49,900 : INFO : PROGRESS: at sentence #200000, processed 1016455 words, keeping 126997 word types\n",
      "2021-11-29 11:56:49,939 : INFO : PROGRESS: at sentence #210000, processed 1113951 words, keeping 129972 word types\n",
      "2021-11-29 11:56:49,974 : INFO : PROGRESS: at sentence #220000, processed 1203031 words, keeping 132421 word types\n",
      "2021-11-29 11:56:50,014 : INFO : PROGRESS: at sentence #230000, processed 1320349 words, keeping 136368 word types\n",
      "2021-11-29 11:56:50,054 : INFO : PROGRESS: at sentence #240000, processed 1438473 words, keeping 139682 word types\n",
      "2021-11-29 11:56:50,094 : INFO : PROGRESS: at sentence #250000, processed 1556655 words, keeping 142644 word types\n",
      "2021-11-29 11:56:50,133 : INFO : PROGRESS: at sentence #260000, processed 1673353 words, keeping 145144 word types\n",
      "2021-11-29 11:56:50,177 : INFO : PROGRESS: at sentence #270000, processed 1808384 words, keeping 146166 word types\n",
      "2021-11-29 11:56:50,220 : INFO : PROGRESS: at sentence #280000, processed 1942731 words, keeping 146739 word types\n",
      "2021-11-29 11:56:50,251 : INFO : PROGRESS: at sentence #290000, processed 2019571 words, keeping 147220 word types\n",
      "2021-11-29 11:56:50,271 : INFO : PROGRESS: at sentence #300000, processed 2071250 words, keeping 147231 word types\n",
      "2021-11-29 11:56:50,292 : INFO : PROGRESS: at sentence #310000, processed 2123327 words, keeping 147237 word types\n",
      "2021-11-29 11:56:50,313 : INFO : PROGRESS: at sentence #320000, processed 2178072 words, keeping 147261 word types\n",
      "2021-11-29 11:56:50,333 : INFO : PROGRESS: at sentence #330000, processed 2231183 words, keeping 147296 word types\n",
      "2021-11-29 11:56:50,353 : INFO : PROGRESS: at sentence #340000, processed 2281196 words, keeping 147319 word types\n",
      "2021-11-29 11:56:50,373 : INFO : PROGRESS: at sentence #350000, processed 2333844 words, keeping 147370 word types\n",
      "2021-11-29 11:56:50,394 : INFO : PROGRESS: at sentence #360000, processed 2387982 words, keeping 147381 word types\n",
      "2021-11-29 11:56:50,418 : INFO : PROGRESS: at sentence #370000, processed 2437476 words, keeping 148703 word types\n",
      "2021-11-29 11:56:50,444 : INFO : PROGRESS: at sentence #380000, processed 2488516 words, keeping 149833 word types\n",
      "2021-11-29 11:56:50,478 : INFO : PROGRESS: at sentence #390000, processed 2570195 words, keeping 150779 word types\n",
      "2021-11-29 11:56:50,514 : INFO : PROGRESS: at sentence #400000, processed 2650972 words, keeping 151378 word types\n",
      "2021-11-29 11:56:50,551 : INFO : PROGRESS: at sentence #410000, processed 2747065 words, keeping 153443 word types\n",
      "2021-11-29 11:56:50,584 : INFO : PROGRESS: at sentence #420000, processed 2835995 words, keeping 154045 word types\n",
      "2021-11-29 11:56:50,614 : INFO : PROGRESS: at sentence #430000, processed 2920784 words, keeping 154343 word types\n",
      "2021-11-29 11:56:50,642 : INFO : PROGRESS: at sentence #440000, processed 3001352 words, keeping 154542 word types\n",
      "2021-11-29 11:56:50,680 : INFO : PROGRESS: at sentence #450000, processed 3102766 words, keeping 155445 word types\n",
      "2021-11-29 11:56:50,707 : INFO : PROGRESS: at sentence #460000, processed 3174807 words, keeping 155587 word types\n",
      "2021-11-29 11:56:50,743 : INFO : PROGRESS: at sentence #470000, processed 3282610 words, keeping 155653 word types\n",
      "2021-11-29 11:56:50,755 : INFO : PROGRESS: at sentence #480000, processed 3302585 words, keeping 155653 word types\n",
      "2021-11-29 11:56:50,764 : INFO : PROGRESS: at sentence #490000, processed 3312708 words, keeping 155653 word types\n",
      "2021-11-29 11:56:50,793 : INFO : PROGRESS: at sentence #500000, processed 3395224 words, keeping 155740 word types\n",
      "2021-11-29 11:56:50,824 : INFO : PROGRESS: at sentence #510000, processed 3487799 words, keeping 155756 word types\n",
      "2021-11-29 11:56:50,860 : INFO : PROGRESS: at sentence #520000, processed 3567376 words, keeping 156984 word types\n",
      "2021-11-29 11:56:50,889 : INFO : PROGRESS: at sentence #530000, processed 3646738 words, keeping 157746 word types\n",
      "2021-11-29 11:56:50,923 : INFO : PROGRESS: at sentence #540000, processed 3748973 words, keeping 157746 word types\n",
      "2021-11-29 11:56:50,937 : INFO : PROGRESS: at sentence #550000, processed 3775929 words, keeping 157746 word types\n",
      "2021-11-29 11:56:50,946 : INFO : PROGRESS: at sentence #560000, processed 3787347 words, keeping 157746 word types\n",
      "2021-11-29 11:56:50,973 : INFO : PROGRESS: at sentence #570000, processed 3859943 words, keeping 157746 word types\n",
      "2021-11-29 11:56:51,004 : INFO : PROGRESS: at sentence #580000, processed 3950236 words, keeping 157746 word types\n",
      "2021-11-29 11:56:51,041 : INFO : PROGRESS: at sentence #590000, processed 4037757 words, keeping 158362 word types\n",
      "2021-11-29 11:56:51,082 : INFO : PROGRESS: at sentence #600000, processed 4144770 words, keeping 160540 word types\n",
      "2021-11-29 11:56:51,117 : INFO : PROGRESS: at sentence #610000, processed 4231944 words, keeping 160927 word types\n",
      "2021-11-29 11:56:51,158 : INFO : PROGRESS: at sentence #620000, processed 4333738 words, keeping 162542 word types\n",
      "2021-11-29 11:56:51,188 : INFO : PROGRESS: at sentence #630000, processed 4406965 words, keeping 163281 word types\n",
      "2021-11-29 11:56:51,217 : INFO : PROGRESS: at sentence #640000, processed 4479269 words, keeping 166015 word types\n",
      "2021-11-29 11:56:51,248 : INFO : PROGRESS: at sentence #650000, processed 4567556 words, keeping 166015 word types\n",
      "2021-11-29 11:56:51,269 : INFO : PROGRESS: at sentence #660000, processed 4619950 words, keeping 166015 word types\n",
      "2021-11-29 11:56:51,278 : INFO : PROGRESS: at sentence #670000, processed 4633927 words, keeping 166015 word types\n",
      "2021-11-29 11:56:51,296 : INFO : PROGRESS: at sentence #680000, processed 4673369 words, keeping 166015 word types\n",
      "2021-11-29 11:56:51,330 : INFO : PROGRESS: at sentence #690000, processed 4777560 words, keeping 166015 word types\n",
      "2021-11-29 11:56:51,367 : INFO : PROGRESS: at sentence #700000, processed 4871940 words, keeping 166040 word types\n",
      "2021-11-29 11:56:51,401 : INFO : PROGRESS: at sentence #710000, processed 4964405 words, keeping 166162 word types\n",
      "2021-11-29 11:56:51,430 : INFO : PROGRESS: at sentence #720000, processed 5046455 words, keeping 166162 word types\n",
      "2021-11-29 11:56:51,450 : INFO : PROGRESS: at sentence #730000, processed 5096824 words, keeping 166162 word types\n",
      "2021-11-29 11:56:51,460 : INFO : PROGRESS: at sentence #740000, processed 5111109 words, keeping 166162 word types\n",
      "2021-11-29 11:56:51,476 : INFO : PROGRESS: at sentence #750000, processed 5146448 words, keeping 166162 word types\n",
      "2021-11-29 11:56:51,509 : INFO : PROGRESS: at sentence #760000, processed 5246907 words, keeping 166163 word types\n",
      "2021-11-29 11:56:51,546 : INFO : PROGRESS: at sentence #770000, processed 5343618 words, keeping 166276 word types\n",
      "2021-11-29 11:56:51,584 : INFO : PROGRESS: at sentence #780000, processed 5455715 words, keeping 166739 word types\n",
      "2021-11-29 11:56:51,622 : INFO : PROGRESS: at sentence #790000, processed 5572993 words, keeping 166751 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:56:51,661 : INFO : PROGRESS: at sentence #800000, processed 5690369 words, keeping 166761 word types\n",
      "2021-11-29 11:56:51,700 : INFO : PROGRESS: at sentence #810000, processed 5808672 words, keeping 166774 word types\n",
      "2021-11-29 11:56:51,730 : INFO : PROGRESS: at sentence #820000, processed 5887884 words, keeping 167572 word types\n",
      "2021-11-29 11:56:51,767 : INFO : PROGRESS: at sentence #830000, processed 5986889 words, keeping 167834 word types\n",
      "2021-11-29 11:56:51,804 : INFO : PROGRESS: at sentence #840000, processed 6091431 words, keeping 170783 word types\n",
      "2021-11-29 11:56:51,841 : INFO : PROGRESS: at sentence #850000, processed 6187964 words, keeping 172114 word types\n",
      "2021-11-29 11:56:51,875 : INFO : PROGRESS: at sentence #860000, processed 6268438 words, keeping 173751 word types\n",
      "2021-11-29 11:56:51,898 : INFO : collected 174111 word types from a corpus of 6320314 raw words and 865656 sentences\n",
      "2021-11-29 11:56:51,899 : INFO : Creating a fresh vocabulary\n",
      "2021-11-29 11:56:52,463 : DEBUG : starting a new internal lifecycle event log for FastText\n",
      "2021-11-29 11:56:52,464 : INFO : FastText lifecycle event {'msg': 'effective_min_count=2 retains 140478 unique words (80.68301256095249%% of original 174111, drops 33633)', 'datetime': '2021-11-29T11:56:52.430094', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-5.0.17-200.fc29.x86_64-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "2021-11-29 11:56:52,465 : INFO : FastText lifecycle event {'msg': 'effective_min_count=2 leaves 6286681 word corpus (99.46785871714602%% of original 6320314, drops 33633)', 'datetime': '2021-11-29T11:56:52.465305', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-5.0.17-200.fc29.x86_64-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "2021-11-29 11:56:53,289 : INFO : deleting the raw counts dictionary of 174111 items\n",
      "2021-11-29 11:56:53,292 : INFO : sample=0.001 downsamples 25 most-common words\n",
      "2021-11-29 11:56:53,292 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 5978706.869132213 word corpus (95.1%% of prior 6286681)', 'datetime': '2021-11-29T11:56:53.292847', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-5.0.17-200.fc29.x86_64-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "2021-11-29 11:56:56,413 : INFO : estimated required memory for 140478 words, 2000000 buckets and 50 dimensions: 559353448 bytes\n",
      "2021-11-29 11:56:56,413 : INFO : resetting layer weights\n",
      "2021-11-29 11:57:07,942 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-11-29T11:57:07.942255', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-5.0.17-200.fc29.x86_64-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n",
      "2021-11-29 11:57:07,943 : INFO : FastText lifecycle event {'msg': 'training model with 20 workers on 140478 vocabulary and 50 features, using sg=1 hs=0 sample=0.001 negative=15 window=5', 'datetime': '2021-11-29T11:57:07.943220', 'gensim': '4.0.1', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-5.0.17-200.fc29.x86_64-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "2021-11-29 11:57:09,199 : INFO : EPOCH 1 - PROGRESS: at 9.71% examples, 85040 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:10,291 : INFO : EPOCH 1 - PROGRESS: at 11.81% examples, 105111 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:11,291 : INFO : EPOCH 1 - PROGRESS: at 15.18% examples, 108164 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:12,353 : INFO : EPOCH 1 - PROGRESS: at 17.29% examples, 112452 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:13,437 : INFO : EPOCH 1 - PROGRESS: at 19.22% examples, 114286 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:14,617 : INFO : EPOCH 1 - PROGRESS: at 20.52% examples, 107921 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:15,710 : INFO : EPOCH 1 - PROGRESS: at 22.58% examples, 112166 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:16,831 : INFO : EPOCH 1 - PROGRESS: at 23.93% examples, 108731 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:17,929 : INFO : EPOCH 1 - PROGRESS: at 25.38% examples, 108308 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:18,945 : INFO : EPOCH 1 - PROGRESS: at 26.75% examples, 110486 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:19,957 : INFO : EPOCH 1 - PROGRESS: at 27.63% examples, 108340 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:21,149 : INFO : EPOCH 1 - PROGRESS: at 29.00% examples, 108703 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:22,195 : INFO : EPOCH 1 - PROGRESS: at 30.08% examples, 108143 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:23,356 : INFO : EPOCH 1 - PROGRESS: at 31.02% examples, 106855 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:24,374 : INFO : EPOCH 1 - PROGRESS: at 32.05% examples, 107260 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:25,402 : INFO : EPOCH 1 - PROGRESS: at 34.52% examples, 109205 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:57:26,469 : INFO : EPOCH 1 - PROGRESS: at 38.29% examples, 112199 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:27,475 : INFO : EPOCH 1 - PROGRESS: at 42.30% examples, 115209 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:28,819 : INFO : EPOCH 1 - PROGRESS: at 43.88% examples, 110941 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:57:29,877 : INFO : EPOCH 1 - PROGRESS: at 46.41% examples, 113363 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:31,015 : INFO : EPOCH 1 - PROGRESS: at 48.33% examples, 113903 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:32,177 : INFO : EPOCH 1 - PROGRESS: at 49.39% examples, 111631 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:33,276 : INFO : EPOCH 1 - PROGRESS: at 51.27% examples, 112508 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:34,279 : INFO : EPOCH 1 - PROGRESS: at 53.81% examples, 112570 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:35,312 : INFO : EPOCH 1 - PROGRESS: at 56.86% examples, 113255 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:36,348 : INFO : EPOCH 1 - PROGRESS: at 58.00% examples, 112502 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:37,477 : INFO : EPOCH 1 - PROGRESS: at 59.51% examples, 112116 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:38,663 : INFO : EPOCH 1 - PROGRESS: at 63.73% examples, 113137 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:39,750 : INFO : EPOCH 1 - PROGRESS: at 65.37% examples, 112582 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:40,784 : INFO : EPOCH 1 - PROGRESS: at 66.82% examples, 112850 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:41,889 : INFO : EPOCH 1 - PROGRESS: at 68.51% examples, 112791 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:42,960 : INFO : EPOCH 1 - PROGRESS: at 69.76% examples, 112020 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:44,005 : INFO : EPOCH 1 - PROGRESS: at 71.37% examples, 112445 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:45,022 : INFO : EPOCH 1 - PROGRESS: at 73.29% examples, 112411 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:57:46,062 : INFO : EPOCH 1 - PROGRESS: at 76.86% examples, 112644 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:47,285 : INFO : EPOCH 1 - PROGRESS: at 78.72% examples, 112310 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:48,315 : INFO : EPOCH 1 - PROGRESS: at 80.19% examples, 112549 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:49,393 : INFO : EPOCH 1 - PROGRESS: at 83.28% examples, 113099 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:50,413 : INFO : EPOCH 1 - PROGRESS: at 85.53% examples, 112655 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:51,489 : INFO : EPOCH 1 - PROGRESS: at 87.40% examples, 113183 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:52,643 : INFO : EPOCH 1 - PROGRESS: at 88.79% examples, 112632 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:53,831 : INFO : EPOCH 1 - PROGRESS: at 90.01% examples, 112207 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:54,873 : INFO : EPOCH 1 - PROGRESS: at 91.29% examples, 112371 words/s, in_qsize 39, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:57:55,902 : INFO : EPOCH 1 - PROGRESS: at 92.47% examples, 112362 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:56,960 : INFO : EPOCH 1 - PROGRESS: at 93.74% examples, 112275 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:58,111 : INFO : EPOCH 1 - PROGRESS: at 94.97% examples, 111410 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:57:58,238 : DEBUG : job loop exiting, total 633 jobs\n",
      "2021-11-29 11:57:59,120 : INFO : EPOCH 1 - PROGRESS: at 96.98% examples, 112526 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-29 11:57:59,992 : DEBUG : worker exiting, processed 35 jobs\n",
      "2021-11-29 11:57:59,992 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-11-29 11:58:00,119 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:58:00,119 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-11-29 11:58:00,136 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,137 : INFO : EPOCH 1 - PROGRESS: at 97.96% examples, 111584 words/s, in_qsize 17, out_qsize 1\n",
      "2021-11-29 11:58:00,138 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-11-29 11:58:00,151 : DEBUG : worker exiting, processed 29 jobs\n",
      "2021-11-29 11:58:00,152 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-11-29 11:58:00,161 : DEBUG : worker exiting, processed 35 jobs\n",
      "2021-11-29 11:58:00,161 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-11-29 11:58:00,216 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:58:00,216 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-11-29 11:58:00,218 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:58:00,219 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-11-29 11:58:00,247 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,247 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-11-29 11:58:00,260 : DEBUG : worker exiting, processed 29 jobs\n",
      "2021-11-29 11:58:00,260 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-29 11:58:00,303 : DEBUG : worker exiting, processed 34 jobs\n",
      "2021-11-29 11:58:00,303 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-29 11:58:00,310 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:58:00,310 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-29 11:58:00,369 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:58:00,370 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-29 11:58:00,397 : DEBUG : worker exiting, processed 34 jobs\n",
      "2021-11-29 11:58:00,398 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-29 11:58:00,409 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,409 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-29 11:58:00,447 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,448 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-29 11:58:00,538 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,538 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-29 11:58:00,551 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,552 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-29 11:58:00,566 : DEBUG : worker exiting, processed 29 jobs\n",
      "2021-11-29 11:58:00,566 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-29 11:58:00,705 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:00,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-29 11:58:00,714 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:58:00,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-29 11:58:00,717 : INFO : EPOCH - 1 : training on 6320314 raw words (5979084 effective words) took 52.8s, 113325 effective words/s\n",
      "2021-11-29 11:58:01,916 : INFO : EPOCH 2 - PROGRESS: at 9.71% examples, 88250 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:02,973 : INFO : EPOCH 2 - PROGRESS: at 11.81% examples, 108923 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:04,114 : INFO : EPOCH 2 - PROGRESS: at 15.18% examples, 106252 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:05,255 : INFO : EPOCH 2 - PROGRESS: at 17.43% examples, 111085 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:06,301 : INFO : EPOCH 2 - PROGRESS: at 19.54% examples, 117049 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:07,443 : INFO : EPOCH 2 - PROGRESS: at 20.66% examples, 108224 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:08,446 : INFO : EPOCH 2 - PROGRESS: at 22.49% examples, 111316 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:09,551 : INFO : EPOCH 2 - PROGRESS: at 23.95% examples, 109257 words/s, in_qsize 39, out_qsize 1\n",
      "2021-11-29 11:58:10,618 : INFO : EPOCH 2 - PROGRESS: at 25.38% examples, 109127 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:11,658 : INFO : EPOCH 2 - PROGRESS: at 26.85% examples, 111875 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:12,779 : INFO : EPOCH 2 - PROGRESS: at 27.73% examples, 108611 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:13,846 : INFO : EPOCH 2 - PROGRESS: at 29.00% examples, 109265 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:14,932 : INFO : EPOCH 2 - PROGRESS: at 30.08% examples, 108358 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:15,958 : INFO : EPOCH 2 - PROGRESS: at 30.94% examples, 107356 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:16,999 : INFO : EPOCH 2 - PROGRESS: at 32.05% examples, 108184 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:58:18,065 : INFO : EPOCH 2 - PROGRESS: at 34.59% examples, 110388 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:19,135 : INFO : EPOCH 2 - PROGRESS: at 38.06% examples, 112276 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:20,154 : INFO : EPOCH 2 - PROGRESS: at 41.84% examples, 114753 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:21,188 : INFO : EPOCH 2 - PROGRESS: at 43.86% examples, 113078 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:22,373 : INFO : EPOCH 2 - PROGRESS: at 46.27% examples, 114321 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:23,394 : INFO : EPOCH 2 - PROGRESS: at 47.53% examples, 113285 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:24,530 : INFO : EPOCH 2 - PROGRESS: at 49.14% examples, 112733 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:25,564 : INFO : EPOCH 2 - PROGRESS: at 51.01% examples, 113497 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:26,608 : INFO : EPOCH 2 - PROGRESS: at 52.35% examples, 112590 words/s, in_qsize 37, out_qsize 2\n",
      "2021-11-29 11:58:27,612 : INFO : EPOCH 2 - PROGRESS: at 56.50% examples, 114123 words/s, in_qsize 40, out_qsize 0\n",
      "2021-11-29 11:58:28,618 : INFO : EPOCH 2 - PROGRESS: at 57.45% examples, 112771 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:29,662 : INFO : EPOCH 2 - PROGRESS: at 59.21% examples, 113693 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:30,677 : INFO : EPOCH 2 - PROGRESS: at 62.24% examples, 113706 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:31,726 : INFO : EPOCH 2 - PROGRESS: at 65.03% examples, 114503 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:32,912 : INFO : EPOCH 2 - PROGRESS: at 66.16% examples, 113264 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:33,934 : INFO : EPOCH 2 - PROGRESS: at 67.69% examples, 113549 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:34,971 : INFO : EPOCH 2 - PROGRESS: at 69.04% examples, 113087 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:36,233 : INFO : EPOCH 2 - PROGRESS: at 71.00% examples, 113341 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:37,324 : INFO : EPOCH 2 - PROGRESS: at 72.96% examples, 113298 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:38,482 : INFO : EPOCH 2 - PROGRESS: at 76.79% examples, 113412 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:39,623 : INFO : EPOCH 2 - PROGRESS: at 78.58% examples, 113286 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:40,646 : INFO : EPOCH 2 - PROGRESS: at 80.09% examples, 113520 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:41,705 : INFO : EPOCH 2 - PROGRESS: at 82.92% examples, 113634 words/s, in_qsize 39, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:58:43,022 : INFO : EPOCH 2 - PROGRESS: at 85.44% examples, 113057 words/s, in_qsize 39, out_qsize 1\n",
      "2021-11-29 11:58:44,095 : INFO : EPOCH 2 - PROGRESS: at 87.51% examples, 113814 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:45,114 : INFO : EPOCH 2 - PROGRESS: at 88.79% examples, 113369 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:46,227 : INFO : EPOCH 2 - PROGRESS: at 90.01% examples, 113115 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:47,317 : INFO : EPOCH 2 - PROGRESS: at 91.09% examples, 112734 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:48,527 : INFO : EPOCH 2 - PROGRESS: at 92.47% examples, 112690 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:49,548 : INFO : EPOCH 2 - PROGRESS: at 94.34% examples, 113463 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:58:50,604 : INFO : EPOCH 2 - PROGRESS: at 95.07% examples, 112215 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:50,749 : DEBUG : job loop exiting, total 633 jobs\n",
      "2021-11-29 11:58:51,723 : INFO : EPOCH 2 - PROGRESS: at 96.98% examples, 112886 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-29 11:58:52,516 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:58:52,517 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-11-29 11:58:52,628 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:58:52,629 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-11-29 11:58:52,635 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:58:52,635 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-11-29 11:58:52,641 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:58:52,641 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-11-29 11:58:52,655 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,655 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-11-29 11:58:52,667 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,667 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-11-29 11:58:52,675 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:58:52,675 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-11-29 11:58:52,700 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,701 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-11-29 11:58:52,723 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,723 : INFO : EPOCH 2 - PROGRESS: at 98.69% examples, 113058 words/s, in_qsize 11, out_qsize 1\n",
      "2021-11-29 11:58:52,725 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-29 11:58:52,752 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,753 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-29 11:58:52,776 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:58:52,777 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-29 11:58:52,781 : DEBUG : worker exiting, processed 27 jobs\n",
      "2021-11-29 11:58:52,781 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-29 11:58:52,783 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,784 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-29 11:58:52,797 : DEBUG : worker exiting, processed 34 jobs\n",
      "2021-11-29 11:58:52,797 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-29 11:58:52,865 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:58:52,865 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-29 11:58:52,982 : DEBUG : worker exiting, processed 34 jobs\n",
      "2021-11-29 11:58:52,982 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-29 11:58:53,020 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:58:53,021 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-29 11:58:53,051 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:58:53,052 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-29 11:58:53,116 : DEBUG : worker exiting, processed 35 jobs\n",
      "2021-11-29 11:58:53,117 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-29 11:58:53,150 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:58:53,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-29 11:58:53,152 : INFO : EPOCH - 2 : training on 6320314 raw words (5979146 effective words) took 52.4s, 114040 effective words/s\n",
      "2021-11-29 11:58:54,359 : INFO : EPOCH 3 - PROGRESS: at 9.71% examples, 88114 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:55,383 : INFO : EPOCH 3 - PROGRESS: at 11.89% examples, 114654 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:56,456 : INFO : EPOCH 3 - PROGRESS: at 15.33% examples, 112312 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:57,563 : INFO : EPOCH 3 - PROGRESS: at 17.41% examples, 114291 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:58,590 : INFO : EPOCH 3 - PROGRESS: at 19.00% examples, 113454 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:58:59,633 : INFO : EPOCH 3 - PROGRESS: at 20.52% examples, 111038 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:00,682 : INFO : EPOCH 3 - PROGRESS: at 22.18% examples, 113041 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:01,732 : INFO : EPOCH 3 - PROGRESS: at 23.81% examples, 111438 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:02,879 : INFO : EPOCH 3 - PROGRESS: at 25.27% examples, 110134 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:03,900 : INFO : EPOCH 3 - PROGRESS: at 26.75% examples, 113025 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:04,915 : INFO : EPOCH 3 - PROGRESS: at 27.34% examples, 108148 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:59:05,987 : INFO : EPOCH 3 - PROGRESS: at 29.00% examples, 111803 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:07,062 : INFO : EPOCH 3 - PROGRESS: at 29.99% examples, 110062 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:08,145 : INFO : EPOCH 3 - PROGRESS: at 31.02% examples, 109802 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:09,215 : INFO : EPOCH 3 - PROGRESS: at 32.05% examples, 109675 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:10,236 : INFO : EPOCH 3 - PROGRESS: at 34.24% examples, 110994 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:11,250 : INFO : EPOCH 3 - PROGRESS: at 37.83% examples, 113754 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:12,279 : INFO : EPOCH 3 - PROGRESS: at 41.59% examples, 116145 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:13,508 : INFO : EPOCH 3 - PROGRESS: at 43.86% examples, 113744 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:14,573 : INFO : EPOCH 3 - PROGRESS: at 46.27% examples, 115596 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:15,586 : INFO : EPOCH 3 - PROGRESS: at 47.71% examples, 115002 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:16,745 : INFO : EPOCH 3 - PROGRESS: at 49.29% examples, 114213 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:17,945 : INFO : EPOCH 3 - PROGRESS: at 51.18% examples, 114547 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:19,019 : INFO : EPOCH 3 - PROGRESS: at 53.81% examples, 114584 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:20,034 : INFO : EPOCH 3 - PROGRESS: at 56.95% examples, 115629 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:21,095 : INFO : EPOCH 3 - PROGRESS: at 58.00% examples, 114331 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:22,191 : INFO : EPOCH 3 - PROGRESS: at 59.51% examples, 113991 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:23,256 : INFO : EPOCH 3 - PROGRESS: at 63.57% examples, 115092 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:24,394 : INFO : EPOCH 3 - PROGRESS: at 65.37% examples, 114586 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:25,413 : INFO : EPOCH 3 - PROGRESS: at 67.08% examples, 115445 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:26,486 : INFO : EPOCH 3 - PROGRESS: at 68.51% examples, 114831 words/s, in_qsize 40, out_qsize 0\n",
      "2021-11-29 11:59:27,548 : INFO : EPOCH 3 - PROGRESS: at 69.85% examples, 114288 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:28,598 : INFO : EPOCH 3 - PROGRESS: at 71.37% examples, 114364 words/s, in_qsize 39, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-29 11:59:29,599 : INFO : EPOCH 3 - PROGRESS: at 73.27% examples, 114327 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:30,669 : INFO : EPOCH 3 - PROGRESS: at 77.35% examples, 114430 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:59:31,722 : INFO : EPOCH 3 - PROGRESS: at 78.72% examples, 114525 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:32,888 : INFO : EPOCH 3 - PROGRESS: at 80.24% examples, 114319 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:33,917 : INFO : EPOCH 3 - PROGRESS: at 83.28% examples, 114970 words/s, in_qsize 37, out_qsize 2\n",
      "2021-11-29 11:59:34,935 : INFO : EPOCH 3 - PROGRESS: at 86.06% examples, 115170 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:36,171 : INFO : EPOCH 3 - PROGRESS: at 87.58% examples, 114769 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:37,215 : INFO : EPOCH 3 - PROGRESS: at 88.89% examples, 114448 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:38,269 : INFO : EPOCH 3 - PROGRESS: at 90.01% examples, 114101 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:39,277 : INFO : EPOCH 3 - PROGRESS: at 91.29% examples, 114312 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:40,278 : INFO : EPOCH 3 - PROGRESS: at 92.47% examples, 114329 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:41,316 : INFO : EPOCH 3 - PROGRESS: at 93.84% examples, 114446 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:42,516 : INFO : EPOCH 3 - PROGRESS: at 95.07% examples, 113402 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:42,560 : DEBUG : job loop exiting, total 633 jobs\n",
      "2021-11-29 11:59:43,539 : INFO : EPOCH 3 - PROGRESS: at 97.08% examples, 114458 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-29 11:59:44,344 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:59:44,345 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-11-29 11:59:44,427 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:59:44,427 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-11-29 11:59:44,457 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:59:44,457 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-11-29 11:59:44,479 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:59:44,480 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-11-29 11:59:44,495 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:59:44,495 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-11-29 11:59:44,503 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:59:44,503 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-11-29 11:59:44,511 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:59:44,511 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-11-29 11:59:44,595 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:59:44,595 : INFO : EPOCH 3 - PROGRESS: at 98.53% examples, 114122 words/s, in_qsize 12, out_qsize 1\n",
      "2021-11-29 11:59:44,597 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-11-29 11:59:44,606 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:59:44,606 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-29 11:59:44,618 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:59:44,619 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-29 11:59:44,683 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:59:44,684 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-29 11:59:44,695 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:59:44,695 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-29 11:59:44,699 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:59:44,700 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-29 11:59:44,722 : DEBUG : worker exiting, processed 32 jobs\n",
      "2021-11-29 11:59:44,722 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-29 11:59:44,782 : DEBUG : worker exiting, processed 33 jobs\n",
      "2021-11-29 11:59:44,782 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-29 11:59:44,868 : DEBUG : worker exiting, processed 30 jobs\n",
      "2021-11-29 11:59:44,868 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-29 11:59:44,874 : DEBUG : worker exiting, processed 29 jobs\n",
      "2021-11-29 11:59:44,874 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-29 11:59:44,880 : DEBUG : worker exiting, processed 36 jobs\n",
      "2021-11-29 11:59:44,880 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-29 11:59:44,981 : DEBUG : worker exiting, processed 34 jobs\n",
      "2021-11-29 11:59:44,981 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-29 11:59:45,036 : DEBUG : worker exiting, processed 31 jobs\n",
      "2021-11-29 11:59:45,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-29 11:59:45,038 : INFO : EPOCH - 3 : training on 6320314 raw words (5978851 effective words) took 51.9s, 115251 effective words/s\n",
      "2021-11-29 11:59:46,276 : INFO : EPOCH 4 - PROGRESS: at 9.71% examples, 85745 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:47,305 : INFO : EPOCH 4 - PROGRESS: at 11.89% examples, 112683 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:59:48,344 : INFO : EPOCH 4 - PROGRESS: at 15.33% examples, 112144 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:49,438 : INFO : EPOCH 4 - PROGRESS: at 17.41% examples, 114427 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:50,501 : INFO : EPOCH 4 - PROGRESS: at 19.43% examples, 118084 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:51,633 : INFO : EPOCH 4 - PROGRESS: at 20.66% examples, 110338 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:52,640 : INFO : EPOCH 4 - PROGRESS: at 22.73% examples, 114420 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:53,704 : INFO : EPOCH 4 - PROGRESS: at 23.93% examples, 111355 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:54,985 : INFO : EPOCH 4 - PROGRESS: at 25.47% examples, 109539 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:56,130 : INFO : EPOCH 4 - PROGRESS: at 27.04% examples, 112024 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 11:59:57,165 : INFO : EPOCH 4 - PROGRESS: at 28.22% examples, 111939 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:58,395 : INFO : EPOCH 4 - PROGRESS: at 29.29% examples, 109510 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 11:59:59,568 : INFO : EPOCH 4 - PROGRESS: at 30.85% examples, 111916 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:00,672 : INFO : EPOCH 4 - PROGRESS: at 31.62% examples, 109554 words/s, in_qsize 38, out_qsize 1\n",
      "2021-11-29 12:00:01,686 : INFO : EPOCH 4 - PROGRESS: at 33.79% examples, 112684 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:02,803 : INFO : EPOCH 4 - PROGRESS: at 37.40% examples, 114753 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:03,810 : INFO : EPOCH 4 - PROGRESS: at 41.16% examples, 117267 words/s, in_qsize 40, out_qsize 0\n",
      "2021-11-29 12:00:05,149 : INFO : EPOCH 4 - PROGRESS: at 43.86% examples, 115077 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:06,238 : INFO : EPOCH 4 - PROGRESS: at 46.31% examples, 116766 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:07,246 : INFO : EPOCH 4 - PROGRESS: at 47.87% examples, 116525 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:08,292 : INFO : EPOCH 4 - PROGRESS: at 48.98% examples, 115423 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:09,310 : INFO : EPOCH 4 - PROGRESS: at 51.01% examples, 116158 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:10,342 : INFO : EPOCH 4 - PROGRESS: at 52.23% examples, 114793 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:11,432 : INFO : EPOCH 4 - PROGRESS: at 56.50% examples, 116266 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:12,455 : INFO : EPOCH 4 - PROGRESS: at 57.20% examples, 114038 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:13,771 : INFO : EPOCH 4 - PROGRESS: at 59.51% examples, 115169 words/s, in_qsize 39, out_qsize 0\n",
      "2021-11-29 12:00:14,934 : INFO : EPOCH 4 - PROGRESS: at 63.68% examples, 116179 words/s, in_qsize 39, out_qsize 0\n"
     ]
    }
   ],
   "source": [
    "train_FastText_model(sub_word = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_FastText_model(sub_word = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_model(sub_word = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v_model(sub_word = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#w2v with and without subword\n",
    "sw_W2V_CBOW = gensim.models.Word2Vec.load(\"./emb_models/subword_wol_W2V_CBOW_50D\")\n",
    "sw_W2V_SG = gensim.models.Word2Vec.load(\"./emb_models/subword_wol_W2V_Skip-gram_50D\")\n",
    "W2V_CBOW = gensim.models.Word2Vec.load(\"./emb_models/wol_W2V_CBOW_50D\")\n",
    "W2V_SG = gensim.models.Word2Vec.load(\"./emb_models/wol_W2V_Skip-gram_50D\")\n",
    "\n",
    "#FastText with and without subword\n",
    "\n",
    "FT_CBOW = gensim.models.Word2Vec.load(\"./emb_models/wol_FastText_CBOW_50D\")\n",
    "FT_SG = gensim.models.Word2Vec.load(\"./emb_models/wol_FastText_Skip-gram_50D\")\n",
    "sw_FT_CBOW = gensim.models.Word2Vec.load(\"./emb_models/subword_wol_FastText_CBOW_50D\")\n",
    "sw_FT_SG = gensim.models.Word2Vec.load(\"./emb_models/subword_wol_FastText_Skip-gram_50D\")\n",
    "\n",
    "#print(sw_W2V_CBOW)\n",
    "#print(sw_FT_CBOW)\n",
    "#sw_FT_CBOW.wv.vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_SG.wv.most_similar(\"kettaa\")  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_FT_CBOW.wv.most_similar(\"kettaa\")  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rock_idx = sw_FT_CBOW.wv.key_to_index[\"lo''o\"]\n",
    "#rock_cnt = sw_FT_CBOW.wv.get_vecattr(\"xossa\", \"count\")  #\n",
    "vocab_len = len(FT_CBOW.wv)  #\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_SG.wv.similar_by_word(\"lo\\'o\")  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"./wol-WordSim-100.txt\", \"r\", encoding=\"utf8\")\n",
    "for d in data:\n",
    "    words = d.split()\n",
    "    print(words[0], words[1], W2V_CBOW.wv.wmdistance(words[0], words[1]))\n",
    "#W2V_CBOW.wv.similar_by_vector(\"oduwa\")  # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"./wol-WordSim-100.txt\", \"r\", encoding=\"utf8\")\n",
    "for d in data:\n",
    "    words = d.split()\n",
    "    print(words[0], words[1], FT_CBOW.wv.similarity(words[0], words[1]))\n",
    "#W2V_CBOW.wv.similar_by_vector(\"oduwa\")  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity for Vector Space Models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "data = open(\"./wol-WordSim-100.txt\", \"r\", encoding=\"utf8\")\n",
    "for d in data:\n",
    "    words = d.split()\n",
    "    a = FT_CBOW.wv[words[0]]\n",
    "    #print(a)\n",
    "    b = FT_CBOW.wv[words[1]]\n",
    "    #print(b)\n",
    "    list1.append(words[2])\n",
    "    c = get_cosine_similarity(a, b)\n",
    "    list2.append(c)\n",
    "    #print (words[0], words[1], words[2], c)\n",
    "print(\"list1\", list1)\n",
    "print(\"list2\", list2)\n",
    "#W2V_CBOW.wv.similar_by_vector(\"oduwa\")  # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.spearmanr(list1, list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman Correlation Coefficient\n",
    "Spearman rank correlation: \n",
    "Spearman rank correlation is a non-parametric test \n",
    "that is used to measure the degree of association between two variables. \n",
    "Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. \n",
    "Correlations of -1 or +1 imply an exact monotonic relationship. \n",
    "Positive correlations imply that as x increases, so does y. \n",
    "Negative correlations imply that as x increases, y decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the spearman's correlation between two variables\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(list1, list2)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.5\n",
    "if p > alpha:\n",
    "    print('Samples are uncorrelated')\n",
    "else:\n",
    "    print('Samples are correlated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_SG.build_vocab(sentences=common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
